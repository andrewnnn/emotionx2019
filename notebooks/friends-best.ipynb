{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# EmotionX\n",
    "Best model for Friends dataset <br>\n",
    "Author: Andrew Nguyen <br>\n",
    "Date: 9/6/2019 \n",
    "\n",
    "Summary: <br>\n",
    "Using the target utterance only (utterance2) <br>\n",
    "-> feature selection using TFIDF <br>\n",
    "-> one hot encoding <br>\n",
    "-> linearSVM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "# OPTIONAL: Load the \"autoreload\" extension so that code can change\n",
    "%load_ext autoreload\n",
    "\n",
    "# OPTIONAL: always reload modules so that as you change code in src, it gets loaded\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Adding project root and custom functions\n",
    "from os import listdir, getcwd\n",
    "from os.path import isfile, join\n",
    "import sys\n",
    "from pathlib import Path\n",
    "\n",
    "def find_root_dir():\n",
    "    # find config.py\n",
    "    path = Path(getcwd()).parent\n",
    "    onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "\n",
    "    while \"config.py\" not in onlyfiles:\n",
    "        path = path.parent\n",
    "        onlyfiles = [f for f in listdir(path) if isfile(join(path, f))]\n",
    "        \n",
    "    return path\n",
    "\n",
    "ROOT_DIR = str(find_root_dir())\n",
    "sys.path.append(ROOT_DIR)\n",
    "\n",
    "from config import get_project_root\n",
    "# custom functs\n",
    "from src.features import build_features\n",
    "from src.visualization.visualize import plot_confusion_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import json\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pickle\n",
    "import copy\n",
    "\n",
    "import random\n",
    "from collections import Counter\n",
    "\n",
    "from nltk import ngrams\n",
    "from nltk.tokenize import TweetTokenizer\n",
    "from nltk.corpus import stopwords\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer, TfidfTransformer\n",
    "from sklearn.preprocessing import Binarizer\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.neural_network import MLPClassifier\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix, precision_score, recall_score, f1_score, accuracy_score, classification_report, balanced_accuracy_score\n",
    "from sklearn.utils.multiclass import unique_labels"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = get_project_root()\n",
    "TRAIN_PATH = ROOT_PATH / \"data/raw/EmotionX2018/friends.train.json\"\n",
    "DEV_PATH = ROOT_PATH / \"data/raw/EmotionX2018/friends.dev.json\"\n",
    "TEST_PATH = ROOT_PATH / \"data/raw/EmotionX2018/friends.test.json\"\n",
    "\n",
    "df_train = build_features.to_df(TRAIN_PATH)\n",
    "df_dev = build_features.to_df(DEV_PATH)\n",
    "df_test = build_features.to_df(TEST_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_train[\"split\"] = \"train\"\n",
    "df_dev[\"split\"] = \"dev\"\n",
    "df_test[\"split\"] = \"test\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# concat df\n",
    "df = pd.concat([df_train, df_dev, df_test], ignore_index=True)\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# simplify emotion labels\n",
    "df = df[df.emotion2.isin([\"neutral\", \"joy\", \"sadness\", \"anger\"])]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocess"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# make nan utterances -> empty strings\n",
    "df.utterance1 = df.utterance1.fillna(\"\")\n",
    "df.utterance3 = df.utterance3.fillna(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# preappend utterance 1 + utterance 2 with number\n",
    "tokenizer = TweetTokenizer(reduce_len=True, preserve_case=False)\n",
    "\n",
    "def preappend(sent, num):\n",
    "    words = tokenizer.tokenize(sent)\n",
    "    \n",
    "    # remove stop words\n",
    "    stopWords = set(stopwords.words('english'))\n",
    "    words = [ w for w in words if w not in stopWords]\n",
    "    \n",
    "    words = [ num + w for w in words]\n",
    "    return words\n",
    "\n",
    "df.utterance1 = df.utterance1.apply(lambda x: preappend(x, \"1_\"))\n",
    "df.utterance2 = df.utterance2.apply(lambda x: preappend(x, \"2_\"))\n",
    "df.utterance3 = df.utterance3.apply(lambda x: preappend(x, \"3_\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append utt1 and utt2 arrays together\n",
    "temp = df[[\"utterance1\", \"utterance2\"]].values.tolist()\n",
    "temp = [ t[0] + t[1] for t in temp]\n",
    "df[\"utterance12\"] = temp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## TFIDF "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get vocab\n",
    "ll = df.utterance12.values.tolist()\n",
    "\n",
    "shared_vocab = set()\n",
    "for l in ll:\n",
    "    shared_vocab.update(l)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_df = pd.DataFrame(columns=list(shared_vocab), index = [\"anger\", \"joy\", \"neutral\", \"sadness\"])\n",
    "counts_df = counts_df.fillna(0)\n",
    "\n",
    "## Counting the number of occurances of each token in the corpus of each class\n",
    "for i, tokens in enumerate(df['utterance12']):\n",
    "    for word in tokens:\n",
    "        if word in shared_vocab:\n",
    "            labl =  df['emotion2'][i]\n",
    "            counts_df[word][labl]+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tfidf = TfidfTransformer()\n",
    "\n",
    "X_tfidf = tfidf.fit_transform(counts_df.values).toarray()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "word_score = []\n",
    "word_emote = []\n",
    "for x in zip(*X_tfidf):\n",
    "    y = [('angry', x[0]), ('joy', x[1]), ('neutral', x[2]), ('sadness', x[3])]\n",
    "    y = sorted(y, key=lambda x: x[1])\n",
    "    word_score.append(y[3][1] - y[2][1])\n",
    "    word_emote.append(y[3][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = list(zip(shared_vocab, word_score, word_emote))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "rank = sorted(rank, key=lambda x:x[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab = [ w[0] for w in rank]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature transform (onehot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.utterance2\n",
    "\n",
    "def do_nothing(tokens):\n",
    "    return tokens\n",
    "\n",
    "#  [1000, 2500, 5000, 10000]:\n",
    "half = int(len(vocab)/2)\n",
    "curvocab = vocab[0:half]\n",
    "\n",
    "freq = CountVectorizer(tokenizer=do_nothing, vocabulary=curvocab, preprocessor=None, lowercase=False)\n",
    "X = freq.fit_transform(X)\n",
    "\n",
    "onehot = Binarizer()\n",
    "X = onehot.fit_transform(X.toarray())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(freq.vocabulary_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Train test split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # X as is from above\n",
    "y = df.emotion2\n",
    "split = df.split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainIdx = split[split.isin([\"train\", \"dev\"])].index.tolist()\n",
    "testIdx = split[split == \"test\"].index.tolist()\n",
    "\n",
    "X_train = [X[i] for i in trainIdx]\n",
    "X_test = [X[i] for i in testIdx]\n",
    "y_train = [y[i] for i in trainIdx]\n",
    "y_test = [y[i] for i in testIdx]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and eval on train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# predict\n",
    "y_pred = clf.predict(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Eval predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# np.set_printoptions(precision=2)\n",
    "\n",
    "plot_confusion_matrix(y_test, y_pred,\n",
    "                      title='Confusion matrix, without normalization')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot normalized confusion matrix\n",
    "plot_confusion_matrix(y_test, y_pred, normalize=True,\n",
    "                      title='Normalized confusion matrix')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(classification_report(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Micro F1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"{:.4f}\".format(f1_score(y_test, y_pred, average='micro')))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit and predict for eval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "clf = LinearSVC(random_state=0)\n",
    "clf.fit(X,y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ROOT_PATH = get_project_root()\n",
    "EVAL_PATH = ROOT_PATH / \"data/raw/eval/friends_eval.json\"\n",
    "df_eval = build_features.to_df(EVAL_PATH)\n",
    "\n",
    "def transform(df):\n",
    "    # make nan utterances -> empty strings\n",
    "    df.utterance1 = df.utterance1.fillna(\"\")\n",
    "\n",
    "    # preappend utterance 1 + utterance 2 with number\n",
    "    tokenizer = TweetTokenizer(reduce_len=True, preserve_case=False)\n",
    "\n",
    "    def preappend(sent, num):\n",
    "        words = tokenizer.tokenize(sent)\n",
    "\n",
    "        # remove stop words\n",
    "        stopWords = set(stopwords.words('english'))\n",
    "        words = [ w for w in words if w not in stopWords]\n",
    "\n",
    "        words = [ num + w for w in words]\n",
    "        return words\n",
    "\n",
    "    df.utterance1 = df.utterance1.apply(lambda x: preappend(x, \"1_\"))\n",
    "    df.utterance2 = df.utterance2.apply(lambda x: preappend(x, \"2_\"))\n",
    "    \n",
    "    # append utt1 and utt2 arrays together\n",
    "    temp = df[[\"utterance1\", \"utterance2\"]].values.tolist()\n",
    "    temp = [ t[0] + t[1] for t in temp]\n",
    "    df[\"utterance12\"] = temp\n",
    "    \n",
    "    return df\n",
    "\n",
    "df_evall = transform(df_eval)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def x(df,vocab):\n",
    "\n",
    "    X = df.utterance2\n",
    "\n",
    "    def do_nothing(tokens):\n",
    "        return tokens\n",
    "\n",
    "    #  [1000, 2500, 5000, 10000]:\n",
    "    half = int(len(vocab)/2)\n",
    "    curvocab = vocab[0:half]\n",
    "\n",
    "    freq = CountVectorizer(tokenizer=do_nothing, vocabulary=curvocab, preprocessor=None, lowercase=False)\n",
    "    X = freq.fit_transform(X)\n",
    "\n",
    "    onehot = Binarizer()\n",
    "    X = onehot.fit_transform(X.toarray())   \n",
    "\n",
    "#     y = df.emotion2\n",
    "    return X\n",
    "\n",
    "X = x(df_evall, vocab)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = clf.predict(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# generate output file\n",
    "EVAL_PATH = ROOT_PATH / \"data/raw/eval/friends_eval.json\"\n",
    "file = EVAL_PATH\n",
    "if file:\n",
    "    with open(file, 'r') as f:\n",
    "        datastore = json.load(f)    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iypred = 0\n",
    "\n",
    "for i in range(len(datastore)):\n",
    "    for j in range(len(datastore[i])):\n",
    "        datastore[i][j][\"emotion\"] = y_pred[iypred]\n",
    "        iypred += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mydets = {\n",
    "    \"name\": \"Andrew Nguyen\",\n",
    "    \"email\": \"andrew.nguyen03@adelaide.edu.au\"\n",
    "}\n",
    "\n",
    "out = [mydets, datastore]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "OUT_PATH = ROOT_PATH / \"data/processed/friends.submission.json\"\n",
    "filename = OUT_PATH\n",
    "if filename:\n",
    "    # Writing JSON data\n",
    "    with open(filename, 'w') as f:\n",
    "        json.dump(out, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eval.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(y_pred)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
